# YOLOR PyTorchå·¥å…·å‡½æ•°

import logging  # å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import math  # å¯¼å…¥æ•°å­¦å‡½æ•°æ¨¡å—
import os  # å¯¼å…¥æ“ä½œç³»ç»ŸåŠŸèƒ½æ¨¡å—
import platform  # å¯¼å…¥å¹³å°è¯†åˆ«æ¨¡å—
import time  # å¯¼å…¥æ—¶é—´å¤„ç†æ¨¡å—
import datetime  # å¯¼å…¥æ—¥æœŸæ—¶é—´æ¨¡å—
import subprocess  # å¯¼å…¥å­è¿›ç¨‹ç®¡ç†æ¨¡å—
from copy import deepcopy  # å¯¼å…¥æ·±æ‹·è´å‡½æ•°
from pathlib import Path  # å¯¼å…¥è·¯å¾„å¤„ç†æ¨¡å—

import torch  # å¯¼å…¥PyTorchåº“
import torch.nn as nn  # å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥å‡½æ•°å¼æ¥å£
import torchvision  # å¯¼å…¥PyTorchè§†è§‰åº“

logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨

def date_modified(path=__file__):
    """
    è¿”å›å¯è¯»çš„æ–‡ä»¶ä¿®æ”¹æ—¥æœŸï¼Œä¾‹å¦‚'2021-3-26'
    
    å‚æ•°:
        path: è¦æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰æ–‡ä»¶
        
    è¿”å›:
        æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²
    """
    t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)
    return f'{t.year}-{t.month}-{t.day}'


def git_describe(path=Path(__file__).parent):  # pathå¿…é¡»æ˜¯ä¸€ä¸ªç›®å½•
    """
    è¿”å›å¯è¯»çš„Gitæè¿°ï¼Œä¾‹å¦‚v5.0-5-g3e25f1e
    
    å‚æ•°:
        path: Gitä»“åº“ç›®å½•è·¯å¾„
        
    è¿”å›:
        Gitæè¿°å­—ç¬¦ä¸²æˆ–ç©ºå­—ç¬¦ä¸²(å¦‚æœä¸æ˜¯Gitä»“åº“)
    """
    s = f'git -C {path} describe --tags --long --always'
    try:
        return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]
    except subprocess.CalledProcessError as e:
        return ''  # ä¸æ˜¯Gitä»“åº“

# é€‰æ‹©è¿è¡Œè®¾å¤‡
def select_device(device='', batch_size=None):
    """
    é€‰æ‹©è¿è¡Œè®¾å¤‡(CPU/GPU)
    
    å‚æ•°:
        device: è®¾å¤‡æ ‡è¯†ç¬¦ï¼Œ'cpu'è¡¨ç¤ºCPUï¼Œ'0'è¡¨ç¤ºç¬¬ä¸€ä¸ªGPUï¼Œ'0,1,2,3'è¡¨ç¤ºå¤šä¸ªGPU
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œç”¨äºæ£€æŸ¥æ˜¯å¦ä¸GPUæ•°é‡å…¼å®¹
        
    è¿”å›:
        torch.deviceå¯¹è±¡
    """
    s = f'YOLOR ğŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # ä¿¡æ¯å­—ç¬¦ä¸²
    cpu = device.lower() == 'cpu'
    if cpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # å¼ºåˆ¶torch.cuda.is_available()è¿”å›False
    elif device:  # å¦‚æœè¯·æ±‚äº†éCPUè®¾å¤‡
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # è®¾ç½®ç¯å¢ƒå˜é‡
        assert torch.cuda.is_available(), f'CUDAä¸å¯ç”¨ï¼Œæ— æ•ˆçš„è®¾å¤‡{device}'  # æ£€æŸ¥å¯ç”¨æ€§

    cuda = not cpu and torch.cuda.is_available()
    if cuda:
        n = torch.cuda.device_count()
        if n > 1 and batch_size:  # æ£€æŸ¥batch_sizeæ˜¯å¦ä¸GPUæ•°é‡å…¼å®¹
            assert batch_size % n == 0, f'æ‰¹å¤„ç†å¤§å°{batch_size}ä¸æ˜¯GPUæ•°é‡{n}çš„å€æ•°'
        space = ' ' * len(s)
        for i, d in enumerate(device.split(',') if device else range(n)):
            p = torch.cuda.get_device_properties(i)
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\n"  # å­—èŠ‚è½¬MB
    else:
        s += 'CPU\n'

    logger.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emojiå®‰å…¨è¾“å‡º
    return torch.device('cuda:0' if cuda else 'cpu')

# è·å–åŒæ­¥çš„æ—¶é—´
def time_synchronized():
    """
    è·å–PyTorchå‡†ç¡®çš„æ—¶é—´(å¦‚æœå¯ç”¨ï¼Œä¼šåŒæ­¥CUDA)
    
    è¿”å›:
        å½“å‰æ—¶é—´æˆ³
    """
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()

# åˆå§‹åŒ–æ¨¡å‹æƒé‡
def initialize_weights(model):
    """
    åˆå§‹åŒ–æ¨¡å‹çš„æƒé‡
    
    å‚æ•°:
        model: è¦åˆå§‹åŒ–çš„æ¨¡å‹
    """
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            m.eps = 1e-3  # è®¾ç½®BatchNorm2dçš„epsilonå€¼
            m.momentum = 0.03  # è®¾ç½®BatchNorm2dçš„åŠ¨é‡å€¼
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6]:
            m.inplace = True  # è®¾ç½®æ¿€æ´»å‡½æ•°ä¸ºå°±åœ°æ“ä½œæ¨¡å¼

# èåˆå·ç§¯å’Œæ‰¹å½’ä¸€åŒ–å±‚
def fuse_conv_and_bn(conv, bn):
    """
    èåˆå·ç§¯å’Œæ‰¹å½’ä¸€åŒ–å±‚ä»¥æé«˜æ¨ç†é€Ÿåº¦
    å‚è€ƒ: https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    
    å‚æ•°:
        conv: å·ç§¯å±‚
        bn: æ‰¹å½’ä¸€åŒ–å±‚
        
    è¿”å›:
        èåˆåçš„å·ç§¯å±‚
    """
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # å‡†å¤‡æ»¤æ³¢å™¨
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv

# æ‰“å°æ¨¡å‹ä¿¡æ¯
def model_info(model, verbose=False, img_size=640):
    """
    æ‰“å°æ¨¡å‹ä¿¡æ¯ã€‚img_sizeå¯ä»¥æ˜¯æ•´æ•°æˆ–åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼šimg_size=640æˆ–img_size=[640, 320]
    
    å‚æ•°:
        model: è¦åˆ†æçš„æ¨¡å‹
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        img_size: è¾“å…¥å›¾åƒå°ºå¯¸
    """
    n_p = sum(x.numel() for x in model.parameters())  # å‚æ•°æ€»æ•°
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # éœ€è¦æ¢¯åº¦çš„å‚æ•°æ€»æ•°
    if verbose:
        print('%5s %40s %9s %12s %20s %10s %10s' % ('å±‚', 'åç§°', 'æ¢¯åº¦', 'å‚æ•°', 'å½¢çŠ¶', 'å‡å€¼', 'æ ‡å‡†å·®'))
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # è®¡ç®—FLOPs(æµ®ç‚¹è¿ç®—æ•°)
        from thop import profile
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32
        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # è¾“å…¥
        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs
        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # å¦‚æœæ˜¯int/floatåˆ™å±•å¼€
        fs = ', %.1f GFLOPs' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 GFLOPs
    except (ImportError, Exception):
        fs = ''

    logger.info(f"æ¨¡å‹æ‘˜è¦: {len(list(model.modules()))}å±‚, {n_p}ä¸ªå‚æ•°, {n_g}ä¸ªæ¢¯åº¦{fs}")

# åŠ è½½åˆ†ç±»å™¨
def load_classifier(name='resnet101', n=2):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é‡å¡‘ä¸ºnç±»è¾“å‡º
    
    å‚æ•°:
        name: æ¨¡å‹åç§°ï¼Œä¾‹å¦‚'resnet101'
        n: è¾“å‡ºç±»åˆ«æ•°
        
    è¿”å›:
        é¢„è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹
    """
    model = torchvision.models.__dict__[name](pretrained=True)

    # ResNetæ¨¡å‹å±æ€§
    # input_size = [3, 224, 224]
    # input_space = 'RGB'
    # input_range = [0, 1]
    # mean = [0.485, 0.456, 0.406]
    # std = [0.229, 0.224, 0.225]

    # é‡å¡‘è¾“å‡ºä¸ºnä¸ªç±»åˆ«
    filters = model.fc.weight.shape[1]
    model.fc.bias = nn.Parameter(torch.zeros(n), requires_grad=True)
    model.fc.weight = nn.Parameter(torch.zeros(n, filters), requires_grad=True)
    model.fc.out_features = n
    return model

# ç¼©æ”¾å›¾åƒ
def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    """
    æŒ‰æ¯”ä¾‹ç¼©æ”¾å›¾åƒï¼Œå—gs-multipleçº¦æŸ
    
    å‚æ•°:
        img: è¾“å…¥å›¾åƒï¼Œå½¢çŠ¶ä¸º(bs,3,y,x)
        ratio: ç¼©æ”¾æ¯”ä¾‹
        same_shape: æ˜¯å¦ä¿æŒç›¸åŒå½¢çŠ¶
        gs: ç½‘æ ¼å¤§å°
        
    è¿”å›:
        ç¼©æ”¾åçš„å›¾åƒ
    """
    if ratio == 1.0:
        return img
    else:
        h, w = img.shape[2:]
        s = (int(h * ratio), int(w * ratio))  # æ–°å¤§å°
        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # è°ƒæ•´å¤§å°
        if not same_shape:  # å¡«å……/è£å‰ªå›¾åƒ
            h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]
        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenetå‡å€¼

# å¤åˆ¶å±æ€§
def copy_attr(a, b, include=(), exclude=()):
    """
    ä»bå¤åˆ¶å±æ€§åˆ°aï¼Œå¯ä»¥é€‰æ‹©åªåŒ…å«[...]å¹¶æ’é™¤[...]
    
    å‚æ•°:
        a: ç›®æ ‡å¯¹è±¡
        b: æºå¯¹è±¡
        include: è¦åŒ…å«çš„å±æ€§åˆ—è¡¨
        exclude: è¦æ’é™¤çš„å±æ€§åˆ—è¡¨
    """
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            setattr(a, k, v)

# æ‰¹å½’ä¸€åŒ–åŸºç±»
class BatchNormXd(torch.nn.modules.batchnorm._BatchNorm):
    """
    é€šç”¨ç»´åº¦æ‰¹å½’ä¸€åŒ–åŸºç±»
    """
    def _check_input_dim(self, input):
        """
        é‡å†™è¾“å…¥ç»´åº¦æ£€æŸ¥æ–¹æ³•
        
        BatchNorm1dã€BatchNorm2dã€BatchNorm3dç­‰ä¹‹é—´å”¯ä¸€çš„åŒºåˆ«æ˜¯è¿™ä¸ªè¢«å­ç±»é‡å†™çš„æ–¹æ³•ã€‚
        è¯¥æ–¹æ³•çš„åŸå§‹ç›®æ ‡æ˜¯è¿›è¡Œå¼ é‡å®Œæ•´æ€§æ£€æŸ¥ã€‚
        å¦‚æœä½ å¯ä»¥ç»•è¿‡è¿™äº›å®Œæ•´æ€§æ£€æŸ¥(ä¾‹å¦‚ï¼Œå¦‚æœä½ ç›¸ä¿¡ä½ çš„æ¨ç†ä¼šæä¾›æ­£ç¡®ç»´åº¦çš„è¾“å…¥)ï¼Œ
        é‚£ä¹ˆä½ å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•æ¥è½»æ¾åœ°ä»SyncBatchNormè½¬æ¢
        (ä¸å¹¸çš„æ˜¯ï¼ŒSyncBatchNormä¸å­˜å‚¨åŸå§‹ç±» - å¦‚æœå®ƒè¿™æ ·åšäº†ï¼Œæˆ‘ä»¬å¯ä»¥è¿”å›æœ€åˆåˆ›å»ºçš„ç±»)
        """
        return

# è½¬æ¢åŒæ­¥æ‰¹å½’ä¸€åŒ–
def revert_sync_batchnorm(module):
    """
    å°†SyncBatchNormè½¬æ¢ä¸ºBatchNormXd
    
    è¿™ä¸å®ƒå°è¯•æ¢å¤çš„å‡½æ•°éå¸¸ç›¸ä¼¼:
    https://github.com/pytorch/pytorch/blob/c8b3686a3e4ba63dc59e5dcfe5db3430df256833/torch/nn/modules/batchnorm.py#L679
    
    å‚æ•°:
        module: è¦è½¬æ¢çš„æ¨¡å—
        
    è¿”å›:
        è½¬æ¢åçš„æ¨¡å—
    """
    module_output = module
    if isinstance(module, torch.nn.modules.batchnorm.SyncBatchNorm):
        new_cls = BatchNormXd
        module_output = BatchNormXd(module.num_features,
                                               module.eps, module.momentum,
                                               module.affine,
                                               module.track_running_stats)
        if module.affine:
            with torch.no_grad():
                module_output.weight = module.weight
                module_output.bias = module.bias
        module_output.running_mean = module.running_mean
        module_output.running_var = module.running_var
        module_output.num_batches_tracked = module.num_batches_tracked
        if hasattr(module, "qconfig"):
            module_output.qconfig = module.qconfig
    for name, child in module.named_children():
        module_output.add_module(name, revert_sync_batchnorm(child))
    del module
    return module_output

# JITè·Ÿè¸ªæ¨¡å‹
class TracedModel(nn.Module):
    """
    ä½¿ç”¨TorchScriptå°†æ¨¡å‹è½¬æ¢ä¸ºè¿½è¸ªæ¨¡å‹ï¼Œç”¨äºæé«˜æ¨ç†æ€§èƒ½
    """
    def __init__(self, model=None, device=None, img_size=(640,640)): 
        """
        åˆå§‹åŒ–è¿½è¸ªæ¨¡å‹
        
        å‚æ•°:
            model: åŸå§‹æ¨¡å‹
            device: è¿è¡Œè®¾å¤‡
            img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        """
        super(TracedModel, self).__init__()
        
        print(" å°†æ¨¡å‹è½¬æ¢ä¸ºè¿½è¸ªæ¨¡å‹... ") 
        self.stride = model.stride  # æ¨¡å‹æ­¥é•¿ [8, 16, 32] for yolor
        self.names = model.names  # ç±»åˆ«åç§°
        self.model = model

        self.model = revert_sync_batchnorm(self.model)  # å°†åŒæ­¥æ‰¹å½’ä¸€åŒ–è½¬æ¢ä¸ºæ ‡å‡†æ‰¹å½’ä¸€åŒ–
        self.model.to('cpu')  # ç§»åŠ¨åˆ°CPU
        self.model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼

        self.detect_layer = self.model.model[-1]  # è·å–æ£€æµ‹å±‚
        self.model.traced = True  # æ ‡è®°ä¸ºå·²è¿½è¸ª
        
        rand_example = torch.rand(1, 3, img_size, img_size)  # åˆ›å»ºéšæœºè¾“å…¥
        
        traced_script_module = torch.jit.trace(self.model, rand_example, strict=False)  # è¿½è¸ªæ¨¡å‹
        #traced_script_module = torch.jit.script(self.model)
        traced_script_module.save("traced_model.pt")  # ä¿å­˜è¿½è¸ªæ¨¡å‹
        print(" è¿½è¸ªæ¨¡å‹å·²ä¿å­˜! ")
        self.model = traced_script_module  # æ›´æ–°æ¨¡å‹ä¸ºè¿½è¸ªåçš„ç‰ˆæœ¬
        self.model.to(device)  # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.detect_layer.to(device)  # ç§»åŠ¨æ£€æµ‹å±‚åˆ°æŒ‡å®šè®¾å¤‡
        print(" æ¨¡å‹è¿½è¸ªå®Œæˆ! \n") 

    def forward(self, x, augment=False, profile=False):
        """
        å‰å‘æ¨ç†å‡½æ•°
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            augment: æ˜¯å¦ä½¿ç”¨å¢å¼º(æœªä½¿ç”¨)
            profile: æ˜¯å¦è¿›è¡Œæ€§èƒ½åˆ†æ(æœªä½¿ç”¨)
            
        è¿”å›:
            æ¨¡å‹è¾“å‡º
        """
        out = self.model(x)  # è¿è¡Œè¿½è¸ªæ¨¡å‹
        out = self.detect_layer(out)  # åº”ç”¨æ£€æµ‹å±‚
        return out